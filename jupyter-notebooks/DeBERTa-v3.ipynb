{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/final_data/train.csv')\n",
    "data = data.rename(columns={'score': 'labels'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(data['labels'].unique()))\n",
    "data['labels'] = data['labels'] - 1\n",
    "# print(sorted(data['labels'].unique()))\n",
    "\n",
    "num_classes = data[\"labels\"].nunique()\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sorted(data['score'].unique()))\n",
    "# data['score'] = data['score'] - 1\n",
    "# # print(sorted(data['score'].unique()))\n",
    "\n",
    "# num_classes = data[\"score\"].nunique()\n",
    "# print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Assuming 'text' and 'label' are column names in your dataset\n",
    "    result = tokenizer(examples['full_text'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    result['labels'] = examples['labels']\n",
    "    return result\n",
    "\n",
    "# Apply the function across the dataset\n",
    "print(data.columns)\n",
    "dataset = Dataset.from_pandas(data.iloc[:,1:])\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_dataset.features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_datasets = tokenized_dataset.train_test_split(test_size=0.2, shuffle=True, seed=46)\n",
    "train_dataset = split_datasets['train']\n",
    "eval_dataset = split_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset.features)\n",
    "print(eval_dataset.features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy = accuracy_score(labels.flatten(), predictions.flatten())\n",
    "    kappa = cohen_kappa_score(labels.flatten(), predictions.flatten(), weights=\"quadratic\")\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'kappa': kappa\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class CosineAnnealingScheduler(TrainerCallback):\n",
    "    \"\"\" Custom LR Scheduler that implements a cosine annealing schedule with warmup. \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, num_warmup_steps, num_training_steps, num_cycles=0.5, last_epoch=-1):\n",
    "        self.num_warmup_steps = num_warmup_steps\n",
    "        self.num_training_steps = num_training_steps\n",
    "        self.num_cycles = num_cycles\n",
    "        self.last_epoch = last_epoch\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def on_step_begin(self, args, state, control, **kwargs):\n",
    "        \"\"\" Called right before a training step in the main training loop. \"\"\"\n",
    "        step = state.global_step\n",
    "        if step < self.num_warmup_steps:\n",
    "            lr_scale = float(step) / float(max(1, self.num_warmup_steps))\n",
    "        else:\n",
    "            progress = float(step - self.num_warmup_steps) / float(max(1, self.num_training_steps - self.num_warmup_steps))\n",
    "            lr_scale = max(0.0, 0.5 * (1.0 + math.cos(math.pi * self.num_cycles * 2.0 * progress)))\n",
    "        \n",
    "        for group in self.optimizer.param_groups:\n",
    "            group['lr'] = lr_scale * group['initial_lr']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class MetricsCallback(TrainerCallback):\n",
    "    \"A callback that stores all intermediate training, validation losses and validation accuracy.\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.training_losses = []\n",
    "        self.validation_losses = []\n",
    "        self.validation_accuracy = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        # Logs appear as a dictionary. Check if loss and eval_loss are in the dictionary and append them.\n",
    "        if 'loss' in logs:\n",
    "            self.training_losses.append(logs['loss'])\n",
    "        if 'eval_loss' in logs:\n",
    "            self.validation_losses.append(logs['eval_loss'])\n",
    "        if 'eval_accuracy' in logs:\n",
    "            self.validation_accuracy.append(logs['eval_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, TrainingArguments, Trainer, AutoModelForSequenceClassification\n",
    "\n",
    "num_epochs = 7\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",     # evaluation is done at the end of each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"kappa\"\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('microsoft/deberta-v3-base', num_labels=num_classes)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "num_training_steps = num_epochs * len(train_dataset) // training_args.per_device_train_batch_size\n",
    "scheduler_callback = CosineAnnealingScheduler(optimizer, num_warmup_steps=100, num_training_steps=num_training_steps)\n",
    "metrics_callback = MetricsCallback()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizers=(optimizer, None),\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[scheduler_callback, metrics_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_losses = metrics_callback.training_losses\n",
    "validation_losses = metrics_callback.validation_losses\n",
    "validation_accuracy = metrics_callback.validation_accuracy\n",
    "\n",
    "print(\"Training Losses:\", training_losses)\n",
    "print(\"Validation Losses:\", validation_losses)\n",
    "print(\"Validation Accuracy:\", validation_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(1,num_epochs+1)),validation_losses, label=\"Validation Loss\")\n",
    "plt.plot(list(range(1,num_epochs+1)),training_losses, label=\"Training Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss Value\")\n",
    "plt.title(\"Loss vs Epoch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_values = [0.761299, 0.809548, 0.798076, 0.810284, 0.817964, 0.805913, 0.807585]\n",
    "\n",
    "\n",
    "plt.plot(list(range(1,num_epochs+1)),kappa_values, label=\"Kappa Score\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Kappa score\")\n",
    "plt.title(\"Kappa Score vs Epoch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "ax1.plot(list(range(1,num_epochs+1)),training_losses, label=\"Training Loss\")\n",
    "ax1.plot(list(range(1,num_epochs+1)),validation_losses, label=\"Validation Loss\")\n",
    "ax1.set_xlabel(\"Epochs\")\n",
    "ax1.set_ylabel(\"Loss Value\")\n",
    "ax1.set_title(\"Loss vs Epoch\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(list(range(1,num_epochs+1)),kappa_values, label=\"Kappa Score\")\n",
    "ax2.set_xlabel(\"Epochs\")\n",
    "ax2.set_ylabel(\"Kappa Score\")\n",
    "ax2.set_title(\"Kappa Score vs Epoch\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.suptitle(\"DeBERTa-v3 Training and Performance Analysis\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5001626,
     "sourceId": 8405285,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "DL_Pytorch",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
